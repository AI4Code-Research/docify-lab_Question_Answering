{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain\n",
    "%pip install -q openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "import numpy as np\n",
    "import openai\n",
    "from os import listdir\n",
    "from os.path import isfile\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embedding for all file in the given folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: str=EMBEDDING_MODEL):\n",
    "    result = openai.Embedding.create(\n",
    "      model=model,\n",
    "      input=text\n",
    "    )\n",
    "    return result[\"data\"][0][\"embedding\"]\n",
    "\n",
    "def compute_doc_embeddings(folder):\n",
    "    \"\"\"\n",
    "    Create an embedding for each file in given folder using the OpenAI Embeddings API.\n",
    "    \n",
    "    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        idx: get_embedding(r['context']) for idx, r in enumerate(folder)\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the similarity between the question and document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_similarity(x , y):\n",
    "    \"\"\"\n",
    "    Returns the similarity between two vectors.\n",
    "    \n",
    "    Because OpenAI Embeddings are normalized to length 1, the cosine similarity is the same as the dot product.\n",
    "    \"\"\"\n",
    "    return np.dot(np.array(x), np.array(y))\n",
    "\n",
    "def order_document_sections_by_query_similarity(query, contexts ):\n",
    "    \"\"\"\n",
    "    Find the query embedding for the supplied query, and compare it against all of the pre-calculated document embeddings\n",
    "    to find the most relevant sections. \n",
    "    \n",
    "    Return the list of document sections, sorted by relevance in descending order.\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    document_similarities = sorted([\n",
    "        (vector_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in contexts.items()\n",
    "    ], reverse=True)\n",
    "    \n",
    "    return document_similarities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the most relevant document to the promt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_given_infor_len = 2500\n",
    "def get_relevant_document(question: str, context_embeddings: dict, folder:dict):\n",
    "    \"\"\"\n",
    "    Fetch relevant \n",
    "    \"\"\"\n",
    "    most_relevant_document_sections = order_document_sections_by_query_similarity(question, context_embeddings)\n",
    "    \n",
    "    chosen_sections = \"\"\n",
    "    chosen_filenames = []\n",
    "\n",
    "     \n",
    "    for _, section_index in most_relevant_document_sections:\n",
    "        chosen_filenames.append(folder[section_index]['name'])\n",
    "        if len(chosen_sections.split(' ')) + len(folder[section_index]['context'].split(' ')) < max_given_infor_len:\n",
    "            chosen_sections += folder[section_index]['context']\n",
    "        else:\n",
    "            max_additional_len = max_given_infor_len - len(chosen_sections.split(' '))\n",
    "            chosen_sections += ' '.join(folder[section_index]['context'].split(' ')[:max_additional_len])\n",
    "            break\n",
    "            \n",
    "            \n",
    "    # Useful diagnostic information\n",
    "    print(f\"Selected {len(chosen_filenames)} file\")\n",
    "    for filename in chosen_filenames:\n",
    "        print(filename)\n",
    "    # print(\"\\n\".join(chosen_sections_indexes))\n",
    "    \n",
    "    \n",
    "    return chosen_sections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 3500\n",
    "def get_all_files(path):\n",
    "    files = []\n",
    "    dirs = [path]\n",
    "    for dir in dirs:\n",
    "        # print(dirs)\n",
    "        for f in listdir(dir):\n",
    "            if '.git' in f:\n",
    "                continue\n",
    "            if isfile(dir +'/'+f):\n",
    "                files.append(dir +'/'+f)\n",
    "            else:\n",
    "                dirs.append(dir +'/'+f)\n",
    "    return files\n",
    "\n",
    "def read_folder(path):\n",
    "    files = get_all_files(path)\n",
    "    context_folder = []\n",
    "    for file in files:\n",
    "        if '.pyc' in file:\n",
    "            continue\n",
    "        if ('.py' or '.txt' or '.sh') in file:\n",
    "            with open(file, 'r') as f:\n",
    "                c = f.read()\n",
    "                c = c.replace('\\n', ' ')\n",
    "                id = 0\n",
    "                while id < len(c):\n",
    "                    context = {}\n",
    "                    context['name'] = file\n",
    "                    if id+max_len < len(c):\n",
    "                        context['context'] = c[id:id+max_len]\n",
    "                        id += max_len\n",
    "                    else:\n",
    "                        context['context'] = c[id:]\n",
    "                        id=len(c)\n",
    "                    context_folder.append(context)\n",
    "    return context_folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"This assistant is capable of answering questions located in a designated folder. \n",
    "By analyzing the context of the question, it can identify the most relevant information and provide an accurate answer.\n",
    "Context: {document}\n",
    "Question: {question_input}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"document\", \"question_input\"], \n",
    "    template=template\n",
    ")\n",
    "\n",
    "\n",
    "chatgpt_chain = LLMChain(\n",
    "    llm=OpenAI(temperature=0), \n",
    "    prompt=prompt, \n",
    "    verbose=False\n",
    ")\n",
    "context_folder = read_folder('./Docify-Lab-AIC/')\n",
    "# for r in context_folder:\n",
    "#     print(r)\n",
    "context_embeddings=compute_doc_embeddings(context_folder)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 4 file\n",
      "./Docify-Lab-AIC//training/generation_encoder_decoder.py\n",
      "./Docify-Lab-AIC//streamlit_app.py\n",
      "./Docify-Lab-AIC//utils/__init__.py\n",
      "./Docify-Lab-AIC//training/generation.py\n",
      "-----\n",
      "Question:  what are the necessary libraries for this repository\n",
      "Answer:\n",
      " The necessary libraries for this repository are json, logging, math, os, sys, time, random, dataclasses, enum, itertools, pathlib, typing, datasets, jax, jax.numpy, transformers, argparse, and prompt.\n"
     ]
    }
   ],
   "source": [
    "question = \"what are the necessary libraries for this repository\"\n",
    "relevant_document = get_relevant_document(question=question,context_embeddings=context_embeddings, folder=context_folder)\n",
    "output = chatgpt_chain.predict(document=relevant_document, question_input=question)\n",
    "print('-----')\n",
    "print('Question: ', question)\n",
    "print('Answer:')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 3 file\n",
      "./Docify-Lab-AIC//streamlit_app.py\n",
      "./Docify-Lab-AIC//streamlit_app.py\n",
      "./Docify-Lab-AIC//streamlit_app.py\n",
      "-----\n",
      "Question:  How can we use expander in streamlit\n",
      "Answer:\n",
      " Streamlit's expander widget allows you to expand and collapse sections of your app. This is useful for displaying additional information or hiding sections of your app that are not relevant to the user. To use the expander widget, you can call the expander() method with the text you want to display as the title of the expander. You can then add the content you want to display when the expander is expanded inside the with statement.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can we use expander in streamlit\"\n",
    "relevant_document = get_relevant_document(question=question,context_embeddings=context_embeddings, folder=context_folder)\n",
    "output = chatgpt_chain.predict(document=relevant_document, question_input=question)\n",
    "print('-----')\n",
    "print('Question: ', question)\n",
    "print('Answer:')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 3 file\n",
      "./Docify-Lab-AIC//training/run_clm_flax.py\n",
      "./Docify-Lab-AIC//training/run_encoder_decoder_flax.py\n",
      "./Docify-Lab-AIC//utils/example.py\n",
      "-----\n",
      "Question:  What does this repository do?\n",
      "Answer:\n",
      " This repository is designed to assist developers with many different coding tasks. By leveraging the latest AI technology, our product helps developers save time and effort, allowing them to focus on creating the best possible code. Now, developers can work more efficiently and effectively than ever before.\n"
     ]
    }
   ],
   "source": [
    "question = \"What does this repository do?\"\n",
    "relevant_document = get_relevant_document(question=question,context_embeddings=context_embeddings, folder=context_folder)\n",
    "output = chatgpt_chain.predict(document=relevant_document, question_input=question)\n",
    "print('-----')\n",
    "print('Question: ', question)\n",
    "print('Answer:')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 4 file\n",
      "./Docify-Lab-AIC//training/run_encoder_decoder_flax.py\n",
      "./Docify-Lab-AIC//streamlit_app.py\n",
      "./Docify-Lab-AIC//main.py\n",
      "./Docify-Lab-AIC//utils/example.py\n",
      "-----\n",
      "Question:  give me the summary sentence for each file in the folder to know what it does \n",
      "Answer:\n",
      " This script is used to fine-tune library models for summarization. It can also be adapted to any sequence to sequence task. It includes functions to load datasets, evaluate, and train models. It also includes functions to generate code, summarize code, detect languages, and perform named entity recognition.\n"
     ]
    }
   ],
   "source": [
    "question = \"give me the summary sentence for each file in the folder to know what it does \"\n",
    "relevant_document = get_relevant_document(question=question,context_embeddings=context_embeddings, folder=context_folder)\n",
    "output = chatgpt_chain.predict(document=relevant_document, question_input=question)\n",
    "print('-----')\n",
    "print('Question: ', question)\n",
    "print('Answer:')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QA4Code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
