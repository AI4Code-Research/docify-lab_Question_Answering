{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain\n",
    "%pip install -q openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "import numpy as np\n",
    "import openai\n",
    "from os import listdir\n",
    "from os.path import isfile\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embedding for all file in the given folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: str=EMBEDDING_MODEL):\n",
    "    result = openai.Embedding.create(\n",
    "      model=model,\n",
    "      input=text\n",
    "    )\n",
    "    return result[\"data\"][0][\"embedding\"]\n",
    "\n",
    "def compute_doc_embeddings(folder):\n",
    "    \"\"\"\n",
    "    Create an embedding for each file in given folder using the OpenAI Embeddings API.\n",
    "    \n",
    "    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        idx: get_embedding(r['context']) for idx, r in enumerate(folder)\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the similarity between the question and document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_similarity(x , y):\n",
    "    \"\"\"\n",
    "    Returns the similarity between two vectors.\n",
    "    \n",
    "    Because OpenAI Embeddings are normalized to length 1, the cosine similarity is the same as the dot product.\n",
    "    \"\"\"\n",
    "    return np.dot(np.array(x), np.array(y))\n",
    "\n",
    "def order_document_sections_by_query_similarity(query, contexts ):\n",
    "    \"\"\"\n",
    "    Find the query embedding for the supplied query, and compare it against all of the pre-calculated document embeddings\n",
    "    to find the most relevant sections. \n",
    "    \n",
    "    Return the list of document sections, sorted by relevance in descending order.\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    document_similarities = sorted([\n",
    "        (vector_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in contexts.items()\n",
    "    ], reverse=True)\n",
    "    \n",
    "    return document_similarities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the most relevant document to the promt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_given_infor_len = 2500\n",
    "def get_relevant_document(question: str, context_embeddings: dict, folder:dict):\n",
    "    \"\"\"\n",
    "    Fetch relevant \n",
    "    \"\"\"\n",
    "    most_relevant_document_sections = order_document_sections_by_query_similarity(question, context_embeddings)\n",
    "    \n",
    "    chosen_sections = \"\"\n",
    "    chosen_filenames = []\n",
    "\n",
    "     \n",
    "    for _, section_index in most_relevant_document_sections:\n",
    "        chosen_filenames.append(folder[section_index]['name'])\n",
    "        if len(chosen_sections.split(' ')) + len(folder[section_index]['context'].split(' ')) < max_given_infor_len:\n",
    "            chosen_sections += folder[section_index]['context']\n",
    "        else:\n",
    "            max_additional_len = max_given_infor_len - len(chosen_sections.split(' '))\n",
    "            chosen_sections += ' '.join(folder[section_index]['context'].split(' ')[:max_additional_len])\n",
    "            break\n",
    "            \n",
    "            \n",
    "    # Useful diagnostic information\n",
    "    print(f\"Selected {len(chosen_filenames)} file\")\n",
    "    for filename in chosen_filenames:\n",
    "        print(filename)\n",
    "    # print(\"\\n\".join(chosen_sections_indexes))\n",
    "    \n",
    "    \n",
    "    return chosen_sections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 3500\n",
    "def get_all_files(path):\n",
    "    files = []\n",
    "    dirs = [path]\n",
    "    for dir in dirs:\n",
    "        # print(dirs)\n",
    "        for f in listdir(dir):\n",
    "            if '.git' in f:\n",
    "                continue\n",
    "            if isfile(dir +'/'+f):\n",
    "                files.append(dir +'/'+f)\n",
    "            else:\n",
    "                dirs.append(dir +'/'+f)\n",
    "    return files\n",
    "\n",
    "def read_folder(path):\n",
    "    files = get_all_files(path)\n",
    "    context_folder = []\n",
    "    for file in files:\n",
    "        if '.pyc' in file:\n",
    "            continue\n",
    "        if ('.py' or '.txt' or '.sh') in file:\n",
    "            with open(file, 'r') as f:\n",
    "                c = f.read()\n",
    "                c = c.replace('\\n', ' ')\n",
    "                id = 0\n",
    "                while id < len(c):\n",
    "                    context = {}\n",
    "                    context['name'] = file\n",
    "                    if id+max_len < len(c):\n",
    "                        context['context'] = c[id:id+max_len]\n",
    "                        id += max_len\n",
    "                    else:\n",
    "                        context['context'] = c[id:]\n",
    "                        id=len(c)\n",
    "                    context_folder.append(context)\n",
    "    return context_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"This assistant is capable of answering questions located in a designated folder. \n",
    "By analyzing the context of the question, it can identify the most relevant information and provide an accurate answer.\n",
    "Context: {document}\n",
    "Question: {question_input}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"document\", \"question_input\"], \n",
    "    template=template\n",
    ")\n",
    "\n",
    "\n",
    "chatgpt_chain = LLMChain(\n",
    "    llm=OpenAI(temperature=0), \n",
    "    prompt=prompt, \n",
    "    verbose=True\n",
    ")\n",
    "context_folder = read_folder('./Docify-Lab-AIC/')\n",
    "# for r in context_folder:\n",
    "#     print(r)\n",
    "context_embeddings=compute_doc_embeddings(context_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 3 file\n",
      "./Docify-Lab-AIC//training/generation_encoder_decoder.py\n",
      "./Docify-Lab-AIC//streamlit_app.py\n",
      "./Docify-Lab-AIC//training/generation.py\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThis assistant is capable of answering questions located in a designated folder. \n",
      "By analyzing the context of the question, it can identify the most relevant information and provide an accurate answer.\n",
      "Context: import json import logging import math import os import sys import time import random from dataclasses import asdict, dataclass, field from enum import Enum from itertools import chain from pathlib import Path from typing import Callable, Optional  import datasets import jax import jax.numpy as jnp import transformers from transformers import (     CONFIG_MAPPING,     FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,     AutoConfig,     AutoTokenizer,     FlaxAutoModelForSeq2SeqLM,     HfArgumentParser,     is_tensorboard_available, ) import argparse from prompt import prompt_dict  def get_args():     parser = argparse.ArgumentParser()     parser.add_argument('--model_name_or_path', type=str)     parser.add_argument('--seed', type=int, default=42)     parser.add_argument('--task', type=str, default='translate')     parser.add_argument('--src_lang', type=str, default='python')     args = parser.parse_args()     return args   def main():     config = AutoConfig.from_pretrained(                 args.model_name_or_path,             )      try:         model = FlaxAutoModelForSeq2SeqLM.from_pretrained(             args.model_name_or_path,             config=config,             seed=args.seed,             dtype=getattr(jnp, 'float32'),         )     except:         model = FlaxAutoModelForSeq2SeqLM.from_pretrained(             args.model_name_or_path,             config=config,             seed=args.seed,             dtype=getattr(jnp, 'float32'),             from_pt= True,         )      tokenizer = AutoTokenizer.from_pretrained(             args.model_name_or_path,     )      input = \"\"\"     def read_csv(csv_path, csv_path):         with open(csv_path, 'r') as csv_file:             data = csv.DictReader(csv_file)         return data     \"\"\" #     input = \"\"\" # static void bubbleSort(int[] arr) {   #     int n = arr.length;   #     int temp = 0;   #     for(int i=0; i < n; i++){   #         for(int j=1; j < (n-i); i++){   #             if(arr[j-1] > arr[j]){   #                 //swap elements   #                 \"\"\"      # input = \"A function that calculate sum of two given integers\" #     input = \"\"\" # public ResponseEntity<Page<ReportResponse>> getAll(ReportFilterRequest request, #         @RequestHeader(value = AUTHORIZATION) String token, #         @RequestParam(name = \"page\", required = false, defaultValue = Constant.PAGINATION.DEFAULT_PAGE_NUMBER) int page, #         @RequestParam(name = \"size\", required = false, defaultValue = Constant.PAGINATION.DEFAULT_PAGE_SIZE) int size, #         @RequestParam(name = \"sortBy\", required = false, defaultValue = Constant.SORT.DEFAULT_SORT_BY) String field, #         @RequestParam(name = \"sortDir\", required = false, defaultValue = Constant.SORT.DEFAULT_SORT_DIRECTION) String direction) { #     token = token.substring(\"Bearer \".length()); #     String username = jwtTokenUtil.getUsernameFromJwt(token); #     Account account = accountService.getAccountByUsername(username);  #     Pageable pageable = PaginationAndSortFactory.getPagable(size, page, field, direction); #     Page<Report> list = reportService.findByStatus(pageable, request, account); #     Page<ReportResponse> responses = list.map(feedbackMapper::mapReportToReportResponse); #     return new ResponseEntity<>(responses, HttpStatus.OK); # } # \"\"\"  #     input = \"\"\" # def min_cost(cost, m, n):  #     tc = [[0 for x in range(C)] for x in range(R)]  #     tc[0][0] = cost[0][0]  #     for i in range(1, m+1):  #         tc[i][0] = tc[i-1][0] + cost[i][0import os import json import numpy as np import requests  import streamlit as st from streamlit_ace import st_ace from utils import * from io import StringIO  import base64 from pathlib import Path  import warnings warnings.simplefilter(\"ignore\", UserWarning)  API = 'http://4.193.50.237:5000/api'  @st.cache_data def load_session():     return requests.Session()   def translate_panel(seed, length):     with st.container():         st.header(\"Code Translation\")                      # Expand description panel          with st.expander('_:pencil: Quickly select demo example below_'):             option = st.selectbox(                 '',                 ('Select Example', 'Example 1', 'Example 2', 'Example 3'),                 key='trans'             )             example_code = \"\"             language = \"python\"             index=0             uploaded_file = st.file_uploader(\"or Upload your file: \", key='trans_file')             if uploaded_file:                 # with open(uploaded_file, encoding='utf8') as f:                 example_code = uploaded_file.read().decode()              if option == 'Example 1':                 example_code = TRANS_EXAMPLE1                 index=1              elif option == \"Example 2\":                 example_code = TRANS_EXAMPLE2                 language = \"java\"                 index=0              elif option == \"Example 3\":                 example_code = TRANS_EXAMPLE3                 language = \"java\"                 index=0          if example_code  == '':             content = st_ace('', placeholder=\"Input your code in HERE\", language='python', auto_update=True, key=\"<trans-code>\")         else:             content = st_ace(example_code, placeholder=\"Input your code in HERE\", language=language, auto_update=True)                  target_lgs = st.selectbox(             '*Select your target translate languages:*',             ('Python', 'Java'),             index=index         )                  if st.button(\"**:repeat: Translate Code**\"):             with st.spinner('Translating...'):                 source_lgs = detect_lang(API, content)                                  if target_lgs == source_lgs:                     st.warning('Selected language can not be equal to source language', icon=\"⚠️\")                     trans_code = content                 else:                     trans_code = request_api(                     url=API,                      data={                         'input': content,                         'task': 'translate',                         'source_lgs': source_lgs,                         'target_lgs': target_lgs,                         'seed': seed,                         'length': length                     },                 )                     trans_code = json.loads(trans_code.text)                     trans_code = trans_code['output']                                  st.write(\"Translated Code\")             st.code(trans_code, language=target_lgs.lower())           def complete_panel(seed, length):     with st.container():         st.header(\"Code Completion\")                      # Expand description panel          with st.expander('_:pencil: Quickly select demo example below_'):             option = st.selectbox(                 '',                 ('Select Example', 'Example 1', 'Example 2', 'Example 3'),                 key='complete'             )             example_code = \"\"              if option == 'Example 1':                 example_code = COM_EXAMPLE1  import json import logging import math import os import sys import time import random from dataclasses import asdict, dataclass, field from enum import Enum from itertools import chain from pathlib import Path from typing import Callable, Optional  import datasets import jax import jax.numpy as jnp import transformers from transformers import (     CONFIG_MAPPING,     FLAX_MODEL_FOR_CAUSAL_LM_MAPPING,     AutoConfig,     AutoTokenizer,     FlaxAutoModelForCausalLM,     HfArgumentParser,     is_tensorboard_available,     set_seed, ) import argparse from prompt import prompt_dict  def get_args():     parser = argparse.ArgumentParser()     parser.add_argument('--model_name_or_path', type=str)     parser.add_argument('--seed', type=int, default=42)     parser.add_argument('--src_lang', type=str, default='python')     parser.add_argument('--task', type=str, default='translate')     args = parser.parse_args()     return args   def main():     config = AutoConfig.from_pretrained(                 args.model_name_or_path,             )      model = FlaxAutoModelForCausalLM.from_pretrained(                 args.model_name_or_path,                 config=config,                 seed=args.seed,                 dtype=getattr(jnp, 'float32'),             )     tokenizer\n",
      "Question: what are the necessary library for this repository\n",
      "Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " The necessary libraries for this repository are json, logging, math, os, sys, time, random, dataclasses, enum, itertools, pathlib, typing, datasets, jax, jax.numpy, transformers, argparse, and prompt.\n"
     ]
    }
   ],
   "source": [
    "question = \"what are the necessary library for this repository\"\n",
    "relevant_document = get_relevant_document(question=question,context_embeddings=context_embeddings, folder=context_folder)\n",
    "output = chatgpt_chain.predict(document=relevant_document, question_input=question)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 3 file\n",
      "./Docify-Lab-AIC//streamlit_app.py\n",
      "./Docify-Lab-AIC//streamlit_app.py\n",
      "./Docify-Lab-AIC//streamlit_app.py\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThis assistant is capable of answering questions located in a designated folder. \n",
      "By analyzing the context of the question, it can identify the most relevant information and provide an accurate answer.\n",
      "Context: import os import json import numpy as np import requests  import streamlit as st from streamlit_ace import st_ace from utils import * from io import StringIO  import base64 from pathlib import Path  import warnings warnings.simplefilter(\"ignore\", UserWarning)  API = 'http://4.193.50.237:5000/api'  @st.cache_data def load_session():     return requests.Session()   def translate_panel(seed, length):     with st.container():         st.header(\"Code Translation\")                      # Expand description panel          with st.expander('_:pencil: Quickly select demo example below_'):             option = st.selectbox(                 '',                 ('Select Example', 'Example 1', 'Example 2', 'Example 3'),                 key='trans'             )             example_code = \"\"             language = \"python\"             index=0             uploaded_file = st.file_uploader(\"or Upload your file: \", key='trans_file')             if uploaded_file:                 # with open(uploaded_file, encoding='utf8') as f:                 example_code = uploaded_file.read().decode()              if option == 'Example 1':                 example_code = TRANS_EXAMPLE1                 index=1              elif option == \"Example 2\":                 example_code = TRANS_EXAMPLE2                 language = \"java\"                 index=0              elif option == \"Example 3\":                 example_code = TRANS_EXAMPLE3                 language = \"java\"                 index=0          if example_code  == '':             content = st_ace('', placeholder=\"Input your code in HERE\", language='python', auto_update=True, key=\"<trans-code>\")         else:             content = st_ace(example_code, placeholder=\"Input your code in HERE\", language=language, auto_update=True)                  target_lgs = st.selectbox(             '*Select your target translate languages:*',             ('Python', 'Java'),             index=index         )                  if st.button(\"**:repeat: Translate Code**\"):             with st.spinner('Translating...'):                 source_lgs = detect_lang(API, content)                                  if target_lgs == source_lgs:                     st.warning('Selected language can not be equal to source language', icon=\"⚠️\")                     trans_code = content                 else:                     trans_code = request_api(                     url=API,                      data={                         'input': content,                         'task': 'translate',                         'source_lgs': source_lgs,                         'target_lgs': target_lgs,                         'seed': seed,                         'length': length                     },                 )                     trans_code = json.loads(trans_code.text)                     trans_code = trans_code['output']                                  st.write(\"Translated Code\")             st.code(trans_code, language=target_lgs.lower())           def complete_panel(seed, length):     with st.container():         st.header(\"Code Completion\")                      # Expand description panel          with st.expander('_:pencil: Quickly select demo example below_'):             option = st.selectbox(                 '',                 ('Select Example', 'Example 1', 'Example 2', 'Example 3'),                 key='complete'             )             example_code = \"\"              if option == 'Example 1':                 example_code = COM_EXAMPLE1  , value=42, min_value=1, step=1)          with col1:          tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(             [\"Code Translation\", \"Test Generation\", \"Code Repair\",              \"Code Summarization\", \"Code Generation\", \"Code Completion\"]         )                  with tab1:             translate_panel(seed, length)         with tab2:             testgen_panel(seed, length)         with tab3:             repair_panel(seed, length)         with tab4:             codesum_panel(seed, length)         with tab5:             codegen_panel(seed, length)         with tab6:             complete_panel(seed, length)          with col2:         c1, c2, c3, c4, c5, c6, c7 = st.columns([1,1,1,1,1,1,1])          with c2:             logo(\"./assets/fpt_logo.png\")         with c4:             logo(\"./assets/AI_center_logo.png\")         with c6:             logo(\"./assets/Docify-Logo.png\")         # st.markdown(SIDEBAR_INFO, unsafe_allow_html=True)           with st.expander(\"What is Docify-Lab?\", expanded=True):             st.markdown(STORY, unsafe_allow_html=True)                  with st.expander(\"Docify Lab - FPT Software AI Center\", expanded=True):             tab_t1, tab_1, tab_t2, tab_t3, tab_t4, tab_t5, tab_t6 = st.tabs(                 [\"About Us\", \"Leader\",\"Member 1\", \"Member 2\",                  \"Member 3\", \"Member 4\", \"Member 5\"],             )              with tab_t1:                 st.markdown(GROUP_INFO, unsafe_allow_html=True)             with tab_1:                 member_infor('./assets/Anh_Nghi_ava.png', LEADER_INFOR)             with tab_t2:                 member_infor('./assets/Nam_ava.png', NAM_INFOR)             with tab_t3:                 member_infor('./assets/Dung_ava.jpg', DUNG_INFOR)             with tab_t4:                 member_infor('./assets/Minh_ava.JPG', MINH_INFOR)             with tab_t5:                 member_infor('./assets/VA_ava.jpg', VA_INFOR)             with tab_t6:                 member_infor('./assets/KA_ava.jpg', KA_INFOR)      if __name__ == '__main__':     main_app() e = st.file_uploader(\"or Upload your file: \", key='gen_file')             if uploaded_file:                 example_code = uploaded_file.read().decode()              if option == 'Example 1':                 example_code = CGEN_EXAMPLE1              if option == \"Example 2\":                 example_code = CGEN_EXAMPLE2              if option == \"Example 3\":                 example_code = CGEN_EXAMPLE3          if example_code  == '':             content = st_ace('', placeholder=\"Input your code in HERE\", auto_update=True, key=\"<gen-code-code>\")         else:             content = st_ace(example_code, placeholder=\"Input your code in HERE\", auto_update=True)                  target_lgs = st.selectbox(             '*Select your target translate languages:*',             ('Python', 'Java'),             index=0,             key=\"gen_select\"         )          if st.button(\"**:hammer: Generate Code**\"):             with st.spinner('Generating . . .'):                 trans_code = request_api(                     url=API,                      data={    \n",
      "Question: How can we use expander in streamlit\n",
      "Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Streamlit's expander widget allows you to expand and collapse sections of your app. This is useful for hiding long sections of code or text that may not be relevant to the user at the moment. To use the expander widget, you can use the st.expander() function. This function takes two arguments: the title of the expander and a boolean value indicating whether the expander should be expanded or collapsed. You can also pass in an optional third argument to specify the initial state of the expander.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can we use expander in streamlit\"\n",
    "relevant_document = get_relevant_document(question=question,context_embeddings=context_embeddings, folder=context_folder)\n",
    "output = chatgpt_chain.predict(document=relevant_document, question_input=question)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 3 file\n",
      "./Docify-Lab-AIC//training/run_clm_flax.py\n",
      "./Docify-Lab-AIC//training/run_encoder_decoder_flax.py\n",
      "./Docify-Lab-AIC//utils/example.py\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThis assistant is capable of answering questions located in a designated folder. \n",
      "By analyzing the context of the question, it can identify the most relevant information and provide an accurate answer.\n",
      "Context: ion sent is the one passed as arguments along with your Python/PyTorch versions.     send_example_telemetry(\"run_clm\", model_args, data_args, framework=\"flax\")      if not os.path.exists(training_args.output_dir):         os.makedirs(training_args.output_dir)      if (         os.path.exists(training_args.output_dir)         and os.listdir(training_args.output_dir)         and training_args.do_train         and not training_args.overwrite_output_dir     ):         raise ValueError(             f\"Output directory ({training_args.output_dir}) already exists and is not empty.\"             \"Use --overwrite_output_dir to overcome.\"         )      # Make one log on every process with the configuration for debugging.     logging.basicConfig(         format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",         datefmt=\"%m/%d/%Y %H:%M:%S\",         level=logging.INFO,     )     # Setup logging, we only want one process per machine to log things on the screen.     logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)     if jax.process_index() == 0:         datasets.utils.logging.set_verbosity_warning()         transformers.utils.logging.set_verbosity_info()     else:         datasets.utils.logging.set_verbosity_error()         transformers.utils.logging.set_verbosity_error()      # Set the verbosity to info of the Transformers logger (on main process only):     logger.info(f\"Training/evaluation parameters {training_args}\")      # Set seed before initializing model.     set_seed(training_args.seed)      # Handle the repository creation     if training_args.push_to_hub:         if training_args.hub_model_id is None:             repo_name = get_full_repo_name(                 Path(training_args.output_dir).absolute().name, token=training_args.hub_token             )         else:             repo_name = training_args.hub_model_id         create_repo(repo_name, exist_ok=True, token=training_args.hub_token)         repo = Repository(training_args.output_dir, clone_from=repo_name, token=training_args.hub_token)      #  Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)     # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/     # (the dataset will be downloaded automatically from the datasets Hub).     #     # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called     # 'text' is found. You can easily tweak this behavior (see below).     #     # In distributed training, the load_dataset function guarantees that only one local process can concurrently     # download the dataset.     dataset_args = {}     all_dataset = {}     assert data_args.train_file is not None          for task in os.listdir(data_args.train_file):         all_dataset[task] = {}         data_files = {}         data_files[\"train\"] = [os.path.join(data_args.train_file, task, file) for file in os.listdir(os.path.join(data_args.train_file, task))]         if data_args.validation_file is not None:             data_files[\"validation\"] = data_args.validation_file         extension = 'json'         # if extension == \"txt\":         #     extension = \"text\"         #     dataset_args[\"keep_linebreaks\"] = data_args.keep_linebreaks         dataset = load_dataset(             extension,             data_files=data_files,             cache_dir=model_args.cache_dir,             **dataset_args         )ng.set_verbosity_warning()         transformers.utils.logging.set_verbosity_info()     else:         datasets.utils.logging.set_verbosity_error()         transformers.utils.logging.set_verbosity_error()      # Set the verbosity to info of the Transformers logger (on main process only):     logger.info(f\"Training/evaluation parameters {training_args}\")      # Handle the repository creation     if training_args.push_to_hub:         if training_args.hub_model_id is None:             repo_name = get_full_repo_name(                 Path(training_args.output_dir).absolute().name, token=training_args.hub_token             )         else:             repo_name = training_args.hub_model_id         create_repo(repo_name, exist_ok=True, token=training_args.hub_token)         repo = Repository(training_args.output_dir, clone_from=repo_name, token=training_args.hub_token)      # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)     # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/     # (the dataset will be downloaded automatically from the datasets Hub).     #     # For CSV/JSON files this script will use the first column for the full texts and the second column for the     # summaries (unless you specify column names for this with the `text_column` and `summary_column` arguments).     #     dataset_args = {}     all_dataset = {}     assert data_args.train_file is not None          for task in os.listdir(data_args.train_file):         if 'raw' in task:             continue         all_dataset[task] = {}         data_files = {}         data_files[\"train\"] = [os.path.join(data_args.train_file, task, file) for file in os.listdir(os.path.join(data_args.train_file, task))]         if data_args.validation_file is not None:             data_files[\"validation\"] = data_args.validation_file         extension = 'json'         dataset = load_dataset(             extension,             data_files=data_files,             cache_dir=model_args.cache_dir,             **dataset_args         )          if len(dataset['train']) < data_args.max_train_samples:             train_id, valid_id = train_test_split(list(range(len(dataset['train']))), test_size= data_args.validation_split_percentage/100)         else:             train_id, valid_id = train_test_split(random.sample(list(range(len(dataset['train']))), data_args.max_train_samples), test_size= data_args.validation_split_percentage/100)          # if data_args.max_train_samples == -1:         #     valid_len = str(data_args.validation_split_percentage) + '%'         #     end_index = \"\"         # else:         #     end_index = data_args.max_train_samples         #     if len(dataset['train']) < data_args.max_train_samples:         #         valid_len = int(data_args.validation_split_percentage/100 * len(dataset['train']))         #         end_index = len(dataset['train'])         #     else:         #         valid_len = int(data_args.max_train_samples * data_args.validation_split_percentage/100)          if \"validation\" not in dataset.keys():             # dataset[\"validation\"] = load_dataset(             #     extension,             #     data_files=data_files,             #     split=f\"train[:{valid_len}]\",             #     cache_dir=model_args.cache_dir,             #     **dataset_args,             # )             # dataset[\"train\"] = load_dataset(             #     extension,             #    SIDEBAR_INFO = \"\"\" <div class=\"contributors font-body text-bold\"> <a class=\"contributor comma\"> NamLH31,</a> <a class=\"contributor comma\">  MinhNA4,</a> <a class=\"contributor comma\">  DungNM31,</a> <a class=\"contributor comma\">  AnhDTV7,</a> <a class=\"contributor comma\">  AnhTVK</a> </div> \"\"\"  LEADER_INFOR = \"\"\" Dr. Bùi Duy Quốc Nghị (NghiBDQ)\\n Chief Scientist of AI4Code team at FPT Software AI Center  \"\"\"  NAM_INFOR = \"\"\" Lê Hải Nam (NamLH31)\\n AI Resident at FPT Software AI Center  \"\"\"  DUNG_INFOR = \"\"\" Nguyễn Mạnh Dũng (DungNM31)\\n AI Resident at FPT Software AI Center  \"\"\"  MINH_INFOR = \"\"\" Nguyễn Anh Mimh (MinhNA4)\\n AI Engineer at FPT Software AI Center  \"\"\"  VA_INFOR = \"\"\" Đậu Thị Vân Anh (AnhDTV7)\\n AI Resident at FPT Software AI Center  \"\"\"  KA_INFOR = \"\"\" Trần Vũ Kim Anh (AnhTVK)\\n Software Developer at FPT Software AI Center  \"\"\"  GROUP_INFO = \"\"\" <div class=\"story-box font-body\"> <p> Docify-Lab is from FPT Software AI Center.\\n We focus on exploring new ways to address a lot of software engineering challenges. </p> \"\"\"  STORY = \"\"\" <div class=\"story-box font-body\"> <p> Docify-Lab is designed to assist developers with many different coding tasks.\\n By leveraging the latest AI technology, our product help developers save time and effort, allowing them to focus on creating the best possible code. \\n Now, developers can work more efficiently and effectively than ever before. </p> \"\"\"   TRANS_EXAMPLE1 = \"\"\" def partition(array, low, high):      pivot = array[high]     i = low - 1       for j in range(low, high):         if array[j] <= pivot:             i = i + 1                          tmp = array[i]             array[i] = array[j]             array[j] = tmp       tmp = array[i + 1]     array[i + 1] = array[high]     array[high] = tmp     return i + 1     def quickSort(array, low, high):     if low < high:         pi = partition(array, low, high)         quickSort(array, low, pi - 1)         quickSort(array, pi + 1, high) \"\"\"  TRANS_EXAMPLE2 = \"\"\"public String getMaxValue(List<Object> list, int first, int last) {     List<Float> floatList = new ArrayList<>();     for (int i = 0; i < list.size(); i++) {         Float prova2 = ((Double) list.get(i)).floatValue();         floatList.add(prova2);     }     float max = Float.MIN_VALUE;     String maxValue = \"\";     for (int i = first; i < last; i++) {         if (floatList.get(i) > max) {             max = floatList.get(i);         }     }     maxValue = String.format(\"%.1f\", max);     return maxValue; } \"\"\"  TRANS_EXAMPLE3 = \"\"\"public class Calculator    {       public int dot(int x, int y){         return x * y;     }      public int add(int x, int y){         add_rs = x + y;         return rs;     }      public int divide(float a, float b){         result = a/b;         return result;     }      public String nothing(){         return \"nothing\";     } } \"\"\"   COM_EXAMPLE1 = \"\"\"def sum2num (a, b):\"\"\"   COM_EXAMPLE2 = \"\"\" def is_prime(n):     for i in range(2,int(n/2)):    \n",
      "Question: What does this repository do?\n",
      "Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " This repository is designed to assist developers with many different coding tasks. It leverages the latest AI technology to help developers save time and effort, allowing them to focus on creating the best possible code.\n"
     ]
    }
   ],
   "source": [
    "question = \"What does this repository do?\"\n",
    "relevant_document = get_relevant_document(question=question,context_embeddings=context_embeddings, folder=context_folder)\n",
    "output = chatgpt_chain.predict(document=relevant_document, question_input=question)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 4 file\n",
      "./Docify-Lab-AIC//training/run_encoder_decoder_flax.py\n",
      "./Docify-Lab-AIC//streamlit_app.py\n",
      "./Docify-Lab-AIC//main.py\n",
      "./Docify-Lab-AIC//utils/example.py\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThis assistant is capable of answering questions located in a designated folder. \n",
      "By analyzing the context of the question, it can identify the most relevant information and provide an accurate answer.\n",
      "Context: #!/usr/bin/env python # coding=utf-8 # Copyright 2021 The HuggingFace Team All rights reserved. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # #     http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. \"\"\" Fine-tuning the library models for summarization. \"\"\" # You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.  import json import logging import math import os import sys import time from dataclasses import asdict, dataclass, field from enum import Enum from functools import partial from pathlib import Path from typing import Callable, Optional import random import datasets import evaluate import jax import jax.numpy as jnp import nltk  # Here to have a nice missing dependency error message early on import numpy as np import optax from datasets import Dataset, load_dataset, concatenate_datasets from filelock import FileLock from flax import jax_utils, traverse_util from flax.jax_utils import pad_shard_unpad, unreplicate from flax.training import train_state from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key from huggingface_hub import Repository, create_repo from tqdm import tqdm from sklearn.model_selection import train_test_split  import transformers from transformers import (     CONFIG_MAPPING,     FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,     AutoConfig,     AutoTokenizer,     FlaxAutoModelForSeq2SeqLM,     HfArgumentParser,     is_tensorboard_available, ) from transformers.utils import get_full_repo_name, is_offline_mode, send_example_telemetry from prompt import prompt_dict  logger = logging.getLogger(__name__)  try:     nltk.data.find(\"tokenizers/punkt\") except (LookupError, OSError):     if is_offline_mode():         raise LookupError(             \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"         )     with FileLock(\".lock\") as lock:         nltk.download(\"punkt\", quiet=True)   MODEL_CONFIG_CLASSES = list(FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING.keys()) MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)   @dataclass class TrainingArguments:     output_dir: str = field(         metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},     )     overwrite_output_dir: bool = field(         default=False,         metadata={             \"help\": (                 \"Overwrite the content of the output directory. \"                 \"Use this to continue training if output_dir points to a checkpoint directory.\"             )         },     )     do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})     do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the dev set.\"})     do_predict: bool = field(default=False, metadata={\"help\": \"Whether to run predictions on the test set.\"})     per_device_train_batch_size: int = field(         default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}     )     per_device_eval_batch_size = st.file_uploader(\"or Upload your file: \", key='gen_file')             if uploaded_file:                 example_code = uploaded_file.read().decode()              if option == 'Example 1':                 example_code = CGEN_EXAMPLE1              if option == \"Example 2\":                 example_code = CGEN_EXAMPLE2              if option == \"Example 3\":                 example_code = CGEN_EXAMPLE3          if example_code  == '':             content = st_ace('', placeholder=\"Input your code in HERE\", auto_update=True, key=\"<gen-code-code>\")         else:             content = st_ace(example_code, placeholder=\"Input your code in HERE\", auto_update=True)                  target_lgs = st.selectbox(             '*Select your target translate languages:*',             ('Python', 'Java'),             index=0,             key=\"gen_select\"         )          if st.button(\"**:hammer: Generate Code**\"):             with st.spinner('Generating . . .'):                 trans_code = request_api(                     url=API,                      data={                         'input': content,                         'task': 'text2code',                         'source_lgs': target_lgs.lower(),                         'target_lgs': None,                         'seed': seed,                         'length': length                     },                 )                                  trans_code = json.loads(trans_code.text)                                  st.write(\"Generated Code\")             code = trans_code['output']             st.code(code, language=target_lgs.lower())       def codesum_panel(seed, length):     with st.container():         st.header(\"Code Summarization\")                      # Expand description panel          with st.expander('_:pencil: Quickly select demo example below_'):             option = st.selectbox(                 '',                 ('Select Example', 'Example 1', 'Example 2', 'Example 3'),                 key='<example-sum>'             )             example_code = \"\"             uploaded_file = st.file_uploader(\"or Upload your file: \", key='sum_file')             if uploaded_file:                 # with open(uploaded_file, encoding='utf8') as f:                 example_code = uploaded_file.read().decode()              if option == 'Example 1':                 example_code = SUM_EXAMPLE1              if option == \"Example 2\":                 example_code = SUM_EXAMPLE2              if option == \"Example 3\":                 example_code = SUM_EXAMPLE3          if example_code  == '':             content = st_ace('', placeholder=\"Input your code in HERE\", language='python', auto_update=True, key=\"<sum-code>\")         else:             content = st_ace(example_code, placeholder=\"Input your code in HERE\", language='python', auto_update=True)                  task = 'code2text'         with st.expander('Advanced Option'):             param = st.checkbox('Summarize with param')             if param:                 task += '_param'         if st.button(\"**:hammer_and_wrench: Summarize Code**\"):              with st.spinner('Summarizing . . .'):                 source_lgs = detect_lang(API, content)                 trans_code = request_api(                     url=API,                      data={                         'input': content,                         'task': task,                         'source_lgs': source_lgs,                         'target_lgs': None,                         'seed': seed,         import os import argparse  from preprocess.preprocessing import *   if __name__ == '__main__':     parser = argparse.ArgumentParser()     # Store config     parser.add_argument(         'data_path',          help='data folder contain file.jsonl or huggingface dataset cache'     )     parser.add_argument(         '--save_path',         type=str,         help=''     )     parser.add_argument(         '--l',         type=str,         help=''     )          # Mode     parser.add_argument(         '--parallel',         action='store_true'     )     parser.add_argument(         '--text2code',          action='store_true'     )     parser.add_argument(         '--lgs_detect',          action='store_true'     )     parser.add_argument(         '--ner',          action='store_true'     )          opt = parser.parse_args()     if opt.parallel:         # with open(opt.data_path, 'r', encoding=\"utf-8\") as f:         #     block = f.read()         # parallel_dataset(block, opt.save_path, opt.l)         create_parallel_dataset(opt.data_path, opt.save_path)              elif opt.text2code:         create_text2code_dataset(opt.data_path, opt.save_path)          elif opt.ner:         create_metareg_dataset(opt.data_path, opt.save_path)          elif opt.lgs_detect:         passSIDEBAR_INFO = \"\"\" <div class=\"contributors font-body text-bold\"> <a class=\"contributor comma\"> NamLH31,</a> <a class=\"contributor comma\">  MinhNA4,</a> <a class=\"contributor comma\">  DungNM31,</a>\n",
      "Question: give me the summary sentence for each file in the folder to know what it does \n",
      "Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " This script is used to fine-tune library models for summarization. It can also be adapted to any sequence to sequence task. It includes functions to load datasets, evaluate models, and generate predictions. It also includes functions to create parallel datasets, text-to-code datasets, and MetaReg datasets.\n"
     ]
    }
   ],
   "source": [
    "question = \"give me the summary sentence for each file in the folder to know what it does \"\n",
    "relevant_document = get_relevant_document(question=question,context_embeddings=context_embeddings, folder=context_folder)\n",
    "output = chatgpt_chain.predict(document=relevant_document, question_input=question)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QA4Code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
